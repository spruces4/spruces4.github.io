<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<link rel="stylesheet" href="/_assets/main.css" />

    <title>第五周作业 - Alfie Cheung</title>
  <link rel="stylesheet" href="/_markdown_plugin_assets/highlight.js/atom-one-light.css" /></head>
  <body>
    <div class="main">
      <nav class="navigation">
        <a href="/">Alfie Cheung</a>
      </nav>
      <article>
        <header>
          <h1 class="article-title">第五周作业</h1>
          <div class="article-info">
            <div>
              <span
                >Created At：<time datetime="1715523269088"
                  >2024-05-12 22:14</time
                ></span
              >
              <span
                >Updated At：<time datetime="1716734670089"
                  >2024-05-26 22:44</time
                ></span
              >
            </div>
            
          </div>
        </header>
        <div class="article-content markdown-body"><nav class="table-of-contents"><ul><li><a href="#keepalived-nginx-高可用">keepalived nginx 高可用</a><ul><li><a href="#创建脚本执行用户">创建脚本执行用户</a></li><li><a href="#nginx代理服务配置">Nginx代理服务配置</a></li><li><a href="#应用服务配置">应用服务配置</a></li><li><a href="#keepalived-服务配置">Keepalived 服务配置</a></li><li><a href="#测试检查">测试检查</a></li></ul></li><li><a href="#keepalived-haproxy-高可用">keepalived haproxy 高可用</a><ul><li><a href="#haproxy-服务配置">HAProxy 服务配置</a></li><li><a href="#应用服务配置-1">应用服务配置</a></li><li><a href="#keepalived-服务配置-1">Keepalived 服务配置</a></li><li><a href="#检查测试">检查测试</a></li></ul></li><li><a href="#redis-集群部署">Redis 集群部署</a><ul><li><a href="#部署">部署</a></li></ul></li><li><a href="#cluster-多主节点分区存放">cluster  (多主节点分区存放)</a><ul><li><a href="#数据分布">数据分布</a></li><li><a href="#搭建部署">搭建部署</a><ul><li><a href="#修改参数">修改参数</a></li><li><a href="#创建集群">创建集群</a></li><li><a href="#验证集群">验证集群</a></li></ul></li><li><a href="#集群命令">集群命令</a></li><li><a href="#读写操作">读写操作</a><ul><li><a href="#写入数据">写入数据</a></li><li><a href="#计算slot">计算slot</a></li><li><a href="#集群方式链接">集群方式链接</a></li><li><a href="#python客户端访问">python客户端访问</a></li></ul></li><li><a href="#管理">管理</a><ul><li><a href="#增加slave节点">增加slave节点</a></li><li><a href="#故障迁移">故障迁移</a></li><li><a href="#集群扩容">集群扩容</a></li><li><a href="#集群缩容">集群缩容</a></li><li><a href="#集群倾斜">集群倾斜</a></li></ul></li></ul></li></ul></nav><h1 id="keepalived-nginx-高可用">keepalived nginx 高可用</h1>
<p>!!! warning SElinux<br />
该配置需要关闭<code>SELINXU</code>，否则会出现 <code>503</code> 告警。<br />
<img src="/_resources/dbf08aa9cd4d4fd9bb217974e8feab11.png" /><br />
!!!</p>
<h2 id="创建脚本执行用户">创建脚本执行用户</h2>
<div><pre class="hljs"><code>useradd -r -s <span class="hljs-regexp">/sbin/</span>nologin keepalived_script
chown keepalived_script:keepalived_script <span class="hljs-regexp">/etc/</span>keepalived/check_nginx.sh
chmod <span class="hljs-number">655</span> <span class="hljs-regexp">/etc/</span>keepalived/check_nginx.sh</code></pre></div>
<h2 id="nginx代理服务配置">Nginx代理服务配置</h2>
<div><pre class="hljs"><code><span class="hljs-regexp">/etc/</span>nginx/nginx.conf

user nginx;
worker_processes auto;
error_log <span class="hljs-regexp">/var/</span>log<span class="hljs-regexp">/nginx/</span>error.log;
pid <span class="hljs-regexp">/run/</span>nginx.pid;

<span class="hljs-comment"># Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.</span>
include <span class="hljs-regexp">/usr/</span>share<span class="hljs-regexp">/nginx/m</span>odules/*.conf;

events {
    worker_connections <span class="hljs-number">1024</span>;
}

http {
    upstream demos {
        server <span class="hljs-number">192.168</span>.<span class="hljs-number">100.103</span>:<span class="hljs-number">80</span> weight=<span class="hljs-number">1</span>;  <span class="hljs-comment"># 后端web服务器</span>
        server <span class="hljs-number">192.168</span>.<span class="hljs-number">100.104</span>:<span class="hljs-number">80</span> weight=<span class="hljs-number">1</span>;  <span class="hljs-comment"># 后端web服务器</span>
    }
    server {
        listen <span class="hljs-number">80</span>;
        location /{
                proxy_pass http:<span class="hljs-regexp">//</span>demos/;
        }
    }
}
~           </code></pre></div>
<h2 id="应用服务配置">应用服务配置</h2>
<div><pre class="hljs"><code>yum -y install nginx
systemctl start nginx 

<span class="hljs-built_in">echo</span> <span class="hljs-string">"&lt;h1&gt;`hostname -I`&lt;/h1&gt;"</span> &gt; /usr/share/nginx/html/index.html</code></pre></div>
<h2 id="keepalived-服务配置">Keepalived 服务配置</h2>
<div><pre class="hljs"><code>! Configuration File for keepalived

<span class="hljs-keyword">global_defs</span> {
   router_id alfie_keepalive_1  <span class="hljs-comment"># 备用节点修改名称</span>
   vrrp_skip_check_adv_addr
   vrrp_garp_interval <span class="hljs-number">0</span>
   vrrp_gna_interval <span class="hljs-number">0</span>
   vrrp_mcast_group4 <span class="hljs-number">224.0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">18</span>
}

<span class="hljs-keyword">vrrp_script</span> <span class="hljs-keyword">check_nginx</span> {
    script <span class="hljs-string">"/etc/keepalived/check_nginx.sh"</span>  <span class="hljs-comment"># 使用 kill 发送 0 信号 确认程序是否工作正常</span>
    interval <span class="hljs-number">1</span>
    weight -<span class="hljs-number">30</span>  <span class="hljs-comment"># 失败后降低 30 权重</span>
    fall <span class="hljs-number">3</span>
    rise <span class="hljs-number">5</span>
    timeout <span class="hljs-number">2</span>
}

<span class="hljs-keyword">vrrp_instance</span> <span class="hljs-keyword">alfie_ka1</span> {
    state MASTER  <span class="hljs-comment"># 备用节点为 BACKUP</span>
    interface ens33
    virtual_router_id <span class="hljs-number">61</span>
    <span class="hljs-literal">priority</span> <span class="hljs-number">100</span>  <span class="hljs-comment"># 备用节点默认权重 80</span>
    advert_int <span class="hljs-number">1</span>
    authentication {
        <span class="hljs-literal">auth_type</span> PASS
        auth_pass alopex
    }
    <span class="hljs-keyword">virtual_ipaddress</span> {
        <span class="hljs-number">192.168</span>.<span class="hljs-number">100.111</span>/<span class="hljs-number">24</span>
    }
    <span class="hljs-keyword">unicast_src_ip</span> 192.168.100.101  <span class="hljs-comment"># 备用节点修改本地IP</span>
    unicast_peer{
        192.168.100.102  <span class="hljs-comment"># 备用节点此处填写主节点IP</span>
    }
    <span class="hljs-keyword">track_script</span> {
        check_nginx
    }
}
include /etc/keepalived/conf.d/*.conf</code></pre></div>
<div><pre class="hljs"><code>chmod a+x /etc/keepalived/check_nginx.sh
<span class="hljs-comment"># 文件 /usr/bin/killall -0 nginx</span>

<span class="hljs-comment">#!/bin/bash</span>
/usr/bin/killall -0 nginx</code></pre></div>
<h2 id="测试检查">测试检查</h2>
<p><img src="/_resources/02644064d4744c069321d380e5dcf46e.png" /></p>
<ul>
<li>
<p>关闭DS1 keepalived</p>
<ul>
<li>地址切换正常<br />
<img src="/_resources/7386e8bfcea140429ef7d280c8768e6e.png" /></li>
<li>应用访问正常<br />
<img src="/_resources/353ef8cfcc7d4909ad78c168887f27f7.png" /></li>
</ul>
</li>
<li>
<p>关闭RS1</p>
<ul>
<li>应用访问正常<br />
<img src="/_resources/4975aad83b4d4d4bb10466d0c26ecbf3.png" /></li>
</ul>
</li>
<li>
<p>健康检查 （恢复DS1 keepalived 关闭 nginx）<br />
<img src="/_resources/bd151b80681648659c49ae8a57c14836.png" /></p>
</li>
</ul>
<h1 id="keepalived-haproxy-高可用">keepalived haproxy 高可用</h1>
<h2 id="haproxy-服务配置">HAProxy 服务配置</h2>
<p>!!! warning 注意事项</p>
<div><pre class="hljs"><code><span class="hljs-comment"># 绑定非自身IP地址</span>
vim /etc/sysctl.conf
net.ipv4.ip_nonlocal_bind = 1

sysctl -p

<span class="hljs-comment"># 在防火墙上放行通对VIP的访问</span>
firewall-cmd --permanent --add-rich-rule=<span class="hljs-string">'rule family="ipv4" destination address="192.168.100.111" accept'</span> 
firewall-cmd --reload</code></pre></div>
<p>!!!</p>
<div><pre class="hljs"><code>global
  daemon
  stats socket /var/lib/haproxy/haproxy.sock mode 600 level admin
  nbthread  5

  user haproxy
  group haproxy
  maxconn 100000

defaults
  option http-keep-alive
  maxconn 100000
  mode http
  timeout connect 300000ms
  timeout client 300000ms
  timeout server 300000ms

frontend demo-web-port
  bind 192.168.100.111:80
  mode http
  use_backend demo-web

backend demo-web
  mode http
  default-server inter 1000 weight 6
  server web2 192.168.100.103:80 weight 2<span class="hljs-built_in"> check </span>addr 192.168.100.103 port 80
  server web3 192.168.100.104:80 weight 2<span class="hljs-built_in"> check </span>addr 192.168.100.104 port 80</code></pre></div>
<h2 id="应用服务配置-2">应用服务配置</h2>
<div><pre class="hljs"><code>yum -y install nginx
systemctl start nginx 

<span class="hljs-built_in">echo</span> <span class="hljs-string">"&lt;h1&gt;`hostname -I`&lt;/h1&gt;"</span> &gt; /usr/share/nginx/html/index.html</code></pre></div>
<h2 id="keepalived-服务配置-2">Keepalived 服务配置</h2>
<div><pre class="hljs"><code>! Configuration File for keepalived

<span class="hljs-keyword">global_defs</span> {
   router_id alfie_keepalive_1  <span class="hljs-comment"># 备用节点修改名称</span>
   vrrp_skip_check_adv_addr
   vrrp_garp_interval <span class="hljs-number">0</span>
   vrrp_gna_interval <span class="hljs-number">0</span>
   vrrp_mcast_group4 <span class="hljs-number">224.0</span>.<span class="hljs-number">0</span>.<span class="hljs-number">18</span>
}

<span class="hljs-keyword">vrrp_script</span> <span class="hljs-keyword">check_haproxy</span> {
    script <span class="hljs-string">"/etc/keepalived/check_haproxy.sh"</span>
    interval <span class="hljs-number">1</span>
    weight -<span class="hljs-number">30</span>
    fall <span class="hljs-number">3</span>
    rise <span class="hljs-number">5</span>
    timeout <span class="hljs-number">2</span>
}

<span class="hljs-keyword">vrrp_instance</span> <span class="hljs-keyword">alfie_ka1</span> {
    state MASTER <span class="hljs-comment"># 备用节点修改为 BACKUP</span>
    interface ens33
    virtual_router_id <span class="hljs-number">61</span>
    <span class="hljs-literal">priority</span> <span class="hljs-number">100</span>  <span class="hljs-comment"># 备用节点修改 80</span>
    advert_int <span class="hljs-number">1</span>
    authentication {
        <span class="hljs-literal">auth_type</span> PASS
        auth_pass alopex
    }
    <span class="hljs-keyword">virtual_ipaddress</span> {
        <span class="hljs-number">192.168</span>.<span class="hljs-number">100.111</span>/<span class="hljs-number">24</span>
    }
    <span class="hljs-keyword">unicast_src_ip</span> 192.168.100.101 <span class="hljs-comment"># 备用节点修改为本地IP</span>
    unicast_peer{
        192.168.100.102   <span class="hljs-comment"># 备用节点填主用节点IP</span>
    }
    <span class="hljs-keyword">track_script</span> {
        check_haproxy
    }
}

include /etc/keepalived/conf.d/*.conf
</code></pre></div>
<div><pre class="hljs"><code><span class="hljs-regexp">/etc/</span>keepalived/check_haproxy.sh
chmod <span class="hljs-number">655</span> <span class="hljs-regexp">/etc/</span>keepalived/check_haproxy.sh
<span class="hljs-comment">#!/bin/bash</span>

<span class="hljs-keyword">if</span> <span class="hljs-regexp">/usr/</span>bin/killall -<span class="hljs-number">0</span> haproxy
then
    <span class="hljs-keyword">exit</span> <span class="hljs-number">0</span>
<span class="hljs-keyword">else</span>
    systemctl restart haproxy
fi</code></pre></div>
<h2 id="检查测试">检查测试</h2>
<p><img src="/_resources/aa5decab6b8c4d828fb3a154308eb261.png" /><br />
<img src="/_resources/7c834cb7de134b1683b558970392f65a.png" /></p>
<ul>
<li>
<p>关闭DS1 (keepalived)</p>
<ul>
<li>地址切换正常<br />
<img src="/_resources/a4f2580fe24b4e779c67ea07eb3b2d6b.png" /></li>
<li>应用访问正常<br />
<img src="/_resources/5c8b13fe78504bd7a986bb2dbd2cc2fc.png" /></li>
</ul>
</li>
<li>
<p>关闭RS1</p>
<ul>
<li>应用访问正常<br />
<img src="/_resources/6129d9ac06564637b7b5c57dbecd1816.png" /></li>
</ul>
</li>
<li>
<p>健康检查（恢复keepalived 关闭DS1 (Haproxy)）<br />
<img src="/_resources/7e40a06eaf5a443fb144d21f35b41832.png" /></p>
</li>
</ul>
<h1 id="redis-集群部署">Redis 集群部署</h1>
<h2 id="部署">部署</h2>
<table>
<thead>
<tr>
<th>主机</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr>
<td>s2</td>
<td>192.168.100.102</td>
</tr>
<tr>
<td>s3</td>
<td>192.168.100.103</td>
</tr>
<tr>
<td>s4</td>
<td>192.168.100.104</td>
</tr>
</tbody>
</table>
<h1 id="cluster-多主节点分区存放">cluster  (多主节点分区存放)</h1>
<h2 id="数据分布">数据分布</h2>
<blockquote>
<p>解决把<code>整个数据</code>集按照分区规则映射<code>到多个节点</code>的问题，每个节点负责整体数据的子集。</p>
</blockquote>
<p>!!! info 分区方式</p>
<ul>
<li>哈希分区
<ul>
<li>离散度好 / 数据分布与业务无关 / 无法顺序访问</li>
<li>代表产品： <code>redis cluster</code>，<code>Cassandra</code>， <code>Dynamo</code></li>
</ul>
</li>
<li>顺序分区
<ul>
<li>离散度容易倾斜 / 数据分布业务相关 / 可以顺序访问</li>
<li>代表产品：<code>Bigtable</code>，<code>HBase</code>，<code>Hypertable</code></li>
</ul>
</li>
</ul>
<p><img src="/_resources/2dbd60324b10451db234e0c59588ea5e.png" /><br />
!!!</p>
<p>!!! abstract 哈希分区规则</p>
<ul>
<li>
<p><strong>节点取余分区</strong></p>
<ul>
<li>
<p><strong>说明</strong><br />
节点取余分区方法通过简单的模运算将数据分布到不同的节点上。</p>
</li>
<li>
<p><strong>算法简述</strong></p>
<ol>
<li>计算数据的哈希值 <code>hash(key)</code>。</li>
<li>用节点的数量 <code>N</code> 对哈希值取模：<code>partition = hash(key) % N</code>。</li>
<li>将数据分配到 <code>partition</code> 对应的节点。</li>
</ol>
</li>
<li>
<p><strong>优点</strong></p>
<ul>
<li><strong>实现简单</strong>：算法非常直观，易于实现。</li>
<li><strong>查询性能好</strong>：在节点数量固定的情况下，查询和插入操作的性能都很好。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong></p>
<ul>
<li><strong>扩展性差</strong>：当节点数量变化（增加或减少）时，大部分数据都需要重新分配，导致大量的数据迁移。</li>
<li><strong>负载不均衡</strong>：如果哈希函数不好，可能导致数据分布不均匀，某些节点负载较高。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>一致性哈希分区</strong></p>
<ul>
<li>
<p><strong>说明</strong><br />
一致性哈希分区方法通过环形哈希空间和节点的虚拟副本来实现数据的分布和负载均衡。</p>
</li>
<li>
<p><strong>算法简述</strong></p>
<ol>
<li>将所有节点映射到一个哈希环上。</li>
<li>计算数据的哈希值 <code>hash(key)</code> 并将其放置在哈希环上。</li>
<li>从数据的位置顺时针找到第一个节点，这个节点就是数据的目标节点。</li>
</ol>
</li>
<li>
<p><strong>优点</strong></p>
<ul>
<li><strong>扩展性好</strong>：当节点增加或减少时，只需要重新分配相邻节点的一部分数据，减少了数据迁移量。</li>
<li><strong>负载均衡</strong>：通过添加虚拟节点，可以改善数据分布的均衡性。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong></p>
<ul>
<li><strong>实现复杂</strong>：相比节点取余分区，一致性哈希的实现要复杂得多。</li>
<li><strong>性能略低</strong>：由于需要在哈希环上查找节点位置，可能会导致性能稍低。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>虚拟槽分区</strong> <code>redis cluster实现</code></p>
<ul>
<li>
<p><strong>说明</strong><br />
虚拟槽分区方法通过将哈希空间划分为固定数量的槽，并将这些槽映射到实际节点上。</p>
</li>
<li>
<p><strong>算法简述</strong></p>
<ol>
<li>将哈希空间分成固定数量的槽（如 <code>M</code> 个）。</li>
<li>计算数据的哈希值 <code>hash(key)</code> 并将其映射到某个槽：<code>slot = hash(key) % M</code>。</li>
<li>将槽映射到实际的节点：<code>partition = slot_to_node(slot)</code>。</li>
</ol>
</li>
<li>
<p><strong>优点</strong></p>
<ul>
<li><strong>扩展性好</strong>：增加或减少节点时，只需重新分配一部分槽到新的节点，数据迁移量适中。</li>
<li><strong>负载均衡</strong>：通过合理地分配槽，可以实现较为均衡的负载分布。</li>
</ul>
</li>
<li>
<p><strong>缺点</strong></p>
<ul>
<li><strong>实现复杂</strong>：需要实现槽到节点的映射管理，增加了系统的复杂度。</li>
<li><strong>管理开销</strong>：需要维护槽和节点之间的映射关系，增加了系统的管理开销。<br />
!!!</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>!!! danger redis虚拟槽位信息<br />
设置有<code>0 ~ 16383</code>的槽，共 <code>16384</code> 个数据单元，每个槽映射一个数据子集。<br />
通过hash函数，将数据存放在不同的槽位中，每个集群的节点保存一部分的槽。</p>
<p><img src="/_resources/90f402256bf144aea7c7d4eeb45220f0.png" /></p>
<blockquote>
<p>集群之间的通信可以保证，最多两次就能命中对应槽所在的节点。<br />
!!!</p>
</blockquote>
<h2 id="搭建部署">搭建部署</h2>
<blockquote>
<p>需要先部署redis集群<br />
<a>0xff redis/脚本使用#个人源安装脚本</a></p>
</blockquote>
<p><img src="/_resources/ae607bd1a778464690c422cac7904d12.png" /></p>
<table>
<thead>
<tr>
<th>主机</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr>
<td>s2</td>
<td>192.168.100.102</td>
</tr>
<tr>
<td>s3</td>
<td>192.168.100.103</td>
</tr>
<tr>
<td>s4</td>
<td>192.168.100.104</td>
</tr>
</tbody>
</table>
<h3 id="修改参数">修改参数</h3>
<div><pre class="hljs"><code><span class="hljs-comment"># 修改配置文件</span>
<span class="hljs-comment"># 以下与cluster相关，按需开启</span>
<span class="hljs-comment"># -e '/# cluster-enabled yes/a cluster-enabled yes' \</span>
<span class="hljs-comment"># -e '/# cluster-config-file nodes-6379.conf/a cluster-config-file nodes-6379.conf' \</span>
<span class="hljs-comment"># -e '/cluster-require-full-coverage yes/a cluster-require-full-coverage no' \</span>

sed -i.bak \
-e <span class="hljs-string">'s/appendonly no/appendonly yes/'</span> \
-e <span class="hljs-string">'/masterauth/a masterauth alopex'</span> \
-e <span class="hljs-string">'s/bind 127.0.0.1/bind 0.0.0.0/'</span>  \
-e <span class="hljs-string">"/# requirepass/a requirepass alopex"</span> \
-e <span class="hljs-string">"/^dir .*/c dir /apps/redis/data/"</span>  \
-e <span class="hljs-string">"/logfile .*/c logfile /apps/redis/log/redis-6379.log"</span> \
-e <span class="hljs-string">"/^pidfile .*/c  pidfile /apps/redis/run/redis_6379.pid"</span> \
-e <span class="hljs-string">'/# cluster-enabled yes/a cluster-enabled yes'</span> \
-e <span class="hljs-string">'/# cluster-config-file nodes-6379.conf/a cluster-config-file /apps/redis/etc/nodes-6379.conf'</span> \
-e <span class="hljs-string">'/cluster-require-full-coverage yes/a cluster-require-full-coverage no'</span> \
/apps/redis/etc/redis.conf

<span class="hljs-comment"># 启动服务</span>
systemctl daemon-reload
systemctl <span class="hljs-built_in">enable</span> --now redis</code></pre></div>
<h3 id="创建集群">创建集群</h3>
<blockquote>
<p>集群功能已开启<br />
<img src="/_resources/c19ae1389de7408e9c222558607c2fd5.png" /></p>
</blockquote>
<blockquote>
<p>cluster 状态启动<br />
<img src="/_resources/b9f1e910b387463a99bb1e4735113051.png" /></p>
</blockquote>
<blockquote>
<p>16379 端口已启动<br />
<img src="/_resources/5fcb26c2c6a64ff4b64a96c08b461439.png" /></p>
</blockquote>
<blockquote>
<p>创建集群</p>
</blockquote>
<div><pre class="hljs"><code><span class="hljs-comment"># 三个节点不创建 slave 节点</span>
redis-cli -a alopex --cluster create 192.168.100.102:6379 192.168.100.103:6379 192.168.100.104:6379</code></pre></div>
<p><img src="/_resources/2ed8a09a9b764eeaa23c15b4952dad8e.png" /></p>
<h3 id="验证集群">验证集群</h3>
<blockquote>
<p>master1 节点查看<br />
<img src="/_resources/151237e673294a41ac73303f0289d04c.png" /></p>
</blockquote>
<blockquote>
<p>node节点状态查看<br />
<img src="/_resources/e51315eabfdd453ba2a6695626ea6781.png" /></p>
</blockquote>
<blockquote>
<p>Cluster info查看<br />
<img src="/_resources/80447bed281243d79e2fddc0b78dd33c.png" /></p>
</blockquote>
<blockquote>
<p>任意节点状态查看<br />
<img src="/_resources/080ae31c141c4219aca5fa734967b25d.png" /></p>
</blockquote>
<blockquote>
<p>查看node-port.conf日志<br />
<img src="/_resources/8a0ec890f9f64b59bf72200041aea0ee.png" /></p>
</blockquote>
<blockquote>
<p>查看对应关系<br />
<img src="/_resources/90b41a2d4dee4869b81e24891a246210.png" /></p>
</blockquote>
<h2 id="集群命令">集群命令</h2>
<div><pre class="hljs"><code><span class="hljs-comment">// 集群(cluster)  </span>
CLUSTER INFO                                打印集群的状态信息  
CLUSTER NODES                               列出集群当前已知的所有节点（node），以及这些节点的相关信息 

<span class="hljs-comment">// 节点(node)  </span>
CLUSTER MEET &lt;ip&gt; &lt;port&gt;                    将ip和port所指定的节点添加到集群当中，让它成为集群的一份子
CLUSTER FORGET &lt;node_id&gt;                    从集群中移除node_id指定的节点
CLUSTER REPLICATE &lt;node_id&gt;                 将当前节点设置为node_id指定节点的从节点
CLUSTER SAVECONFIG                          将当前节点的配置信息手动保存到硬盘（nodes-port.conf） 
CLUSTER SLAVES &lt;master_node_id&gt;             查询指定的master_node_id主节点有哪些从（slave）节点

<span class="hljs-comment">// 槽(slot)  </span>
CLUSTER ADDSLOTS &lt;slot&gt; [slot ...]          将一个或多个槽（slot）指派（assign）给当前节点
CLUSTER DELSLOTS &lt;slot&gt; [slot ...]          将一个或多个槽从当前节点移除
CLUSTER FLUSHSLOTS                          移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点
CLUSTER SETSLOT &lt;slot&gt; NODE &lt;node_id&gt;       将当前节点指定的槽（slot）指派给node_id指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽，然后再进行指派
CLUSTER SETSLOT &lt;slot&gt; MIGRATING &lt;node_id&gt;  将当前节点指定的槽（slot）迁移到node_id指定的节点中
CLUSTER SETSLOT &lt;slot&gt; IMPORTING &lt;node_id&gt;  从node_id指定节点中的槽（slot）导入到当前节点
CLUSTER SETSLOT &lt;slot&gt; STABLE               取消对当前节点指定槽（slot）的导入（import）或者迁移（migrate）
CLUSTER SLOTS                               查看槽（slot）在集群中的分配情况

<span class="hljs-comment">// 键 (key)  </span>
CLUSTER KEYSLOT &lt;key&gt;                       计算键key应该被分配在哪个槽上  
CLUSTER COUNTKEYSINSLOT &lt;slot&gt;              返回指定槽（slot）保存key的数量  
CLUSTER GETKEYSINSLOT &lt;slot&gt; &lt;count&gt;        获取指定槽（slot）中count个key，如果指定槽中大于count个key，则只返回前<span class="hljs-built_in">cout</span>个key，小于或为空，则返回最多数量的key </code></pre></div>
<h2 id="读写操作">读写操作</h2>
<h3 id="写入数据">写入数据</h3>
<ul>
<li>
<p>流程图<br />
<img src="/_resources/0813f71424fe4908a017f95c67848f24.png" /></p>
</li>
<li>
<p>写入实例<br />
<img src="/_resources/a2ab59b781f94d9aaf7fda7dfa4fd79f.png" /></p>
</li>
</ul>
<h3 id="计算slot">计算slot</h3>
<ul>
<li><code>keyslot KEY</code><br />
<img src="/_resources/a6c0c8b71c8c49b7b7a271828b1b2e3f.png" /></li>
</ul>
<h3 id="集群方式链接">集群方式链接</h3>
<ul>
<li><code>-c</code> 自动实现slot重定向<br />
<img src="/_resources/7370510502b14c0fbd2040d5ef610fef.png" /></li>
</ul>
<h3 id="python客户端访问">python客户端访问</h3>
<blockquote>
<p>需要先使用 pip 安装 <code>redis-py-cluster</code><br />
<a>0xff redis/脚本使用#cluster调用</a></p>
</blockquote>
<blockquote>
<p>测试结果可见分布较为平均<br />
<img src="/_resources/7c39be6a5a8841c6b824a1fec908fca9.png" /></p>
</blockquote>
<h2 id="管理">管理</h2>
<h3 id="增加slave节点">增加slave节点</h3>
<ol>
<li><a>0xff redis/脚本使用#源安装脚本</a></li>
<li><a title="#%E4%BF%AE%E6%94%B9%E5%8F%82%E6%95%B0" href="#%E4%BF%AE%E6%94%B9%E5%8F%82%E6%95%B0">修改参数</a></li>
<li>查询master节点的 master-id
<blockquote>
<p>redis-cli -h 127.0.0.1 -a alopex --no-auth-warning cluster nodes<br />
<img src="/_resources/6ac6c0e35a8543a38119a695af3bf404.png" /></p>
</blockquote>
</li>
<li>添加 slave 节点<div><pre class="hljs"><code>redis-cli -a &lt;auth-passwd&gt; \
--cluster add-node &lt;current-slave-node:6379&gt; &lt;oneof-cluster-node:6379&gt; \
--cluster-slave --cluster-master-id &lt;cluster-master-id&gt;</code></pre></div>
<img src="/_resources/1187f064af4640d69ca023d2ab1746e7.png" /></li>
<li>确认新增 slave<br />
<img src="/_resources/23df5f2ea33541618fe07d4b6321ff4b.png" /><br />
<img src="/_resources/1c6d4bba41c04581bbcca5c67f6d808b.png" /></li>
</ol>
<h3 id="故障迁移">故障迁移</h3>
<ul>
<li>
<p>全master集群 （无容错能力，节点故障后数据丢失）<br />
<img src="/_resources/fb1ec9a577e44a59bd1fa8b39230b678.png" /></p>
<ul>
<li>需要实现故障迁移，应在设计至少使用<code>6台主机</code>或为其增加<code>slave</code>节点<div><pre class="hljs"><code>redis-cli -a <span class="hljs-number">123456</span> --cluster-replicas \
--cluster-replicas \
--cluster create <span class="hljs-number">10.0.0.18</span>:<span class="hljs-number">6379 10.0</span>.<span class="hljs-number">0.28:6379</span> <span class="hljs-number">10.0.0.58</span>:<span class="hljs-number">6379</span> \
<span class="hljs-number">10.0.0.8</span>:<span class="hljs-number">6379 10.0</span>.<span class="hljs-number">0.38:6379</span> <span class="hljs-number">10.0.0.48</span>:<span class="hljs-number">6379</span></code></pre></div>
</li>
</ul>
</li>
<li>
<p>拥有slave节点的master节点故障 (拥有容错能力，主节点故障后slave节点会顶替工作)</p>
<ul>
<li><img src="/_resources/877139df2eb8409bb45bec622a9c1c16.png" /></li>
<li><img src="/_resources/869d89fecd23456fb239298271015218.png" /></li>
</ul>
</li>
</ul>
<h3 id="集群扩容">集群扩容</h3>
<p>!!! success 场景思路</p>
<blockquote>
<p>场景：现有的Redis cluster架构已经无法满足越来越高的并发访问请求，需要添加新的主机提高并发能力。<br />
思路：先添加节点，在扩展槽位，最后为master加上slave。<br />
!!!</p>
</blockquote>
<p>!!! warning master数量<br />
注意: 生产环境一般建议master节点为<code>奇数</code>个,比如:<code>3,5,7</code>,以防止脑裂现象</p>
<p>&lt; 本地为实验环境，为方便操作仅添加一台master，凑成4台master。&gt;<br />
!!!</p>
<ul>
<li>步骤图解<br />
<img src="/_resources/379e373510bb4987a7e2364c8517e651.png" /><br />
<img src="/_resources/856df059cbb746b5b75831d9a0954e58.png" /></li>
</ul>
<p>!!! info 实际操作<br />
0. 查看当前 <code>slot</code> 分配信息<br />
<img src="/_resources/2ef43085795c4773a1a4c37435dd6a22.png" /><br />
2. 准备新的redis节点，配置使用cluster配置<br />
<a>0xff redis/脚本使用#个人源安装脚本</a><br />
<a title="#%E4%BF%AE%E6%94%B9%E5%8F%82%E6%95%B0" href="#%E4%BF%AE%E6%94%B9%E5%8F%82%E6%95%B0">修改参数</a><br />
2. 添加新的master节点到集群<br />
<code> 	redis-cli -a &lt;auth-passwd&gt; --cluster add-node &lt;current-node-ip:6379&gt; &lt;oneof-cluster-node-ip:6379&gt; 	</code><br />
<img src="/_resources/58c028a04a924d6399c6442fb098cbc6.png" /><br />
3. 检查是新节点是否连接<br />
&gt; redis-cli -h 192.168.100.102 -a alopex --no-auth-warning --cluster info 192.168.100.101:6379<br />
<img src="/_resources/abf41bb7073042468bc3764b678ae164.png" /><br />
4. 重新分配 <code>slot</code> （仅5+版本支持在线扩容，旧版本需要备份、清空槽数据、再恢复）<br />
<code> 	redis-cli -a &lt;auth-passwd&gt; --cluster reshard &lt;oneof-cluster-node-ip&gt;:6379 	</code><br />
* slot数量<br />
* 需要分配的 <code>slot</code> 数量 (16384/master个数)<br />
* 接收slot的ID<br />
* 新的master的ID (没有<code>slot</code>的主机，新加入的主机)<br />
* 接收slot的策略 (新增选择 <code>ALL</code>)<br />
* <strong>使用 "all"</strong>：当你想要将集群中所有现有节点都指定为哈希槽的源节点时，你会使用 "all"。<br />
* 这在你<code>向集群添加新节点时很有用</code>，你希望现有节点将它们的哈希槽<code>均匀地分配给它们自己和新节点</code>。<br />
* <strong>使用 "done"</strong>：当你<code>完成手动输入所有源节点ID的任务</code>时，你会使用 "done"。<br />
* 这表示你已经完成了指定哈希槽重新分配的源节点的任务。<br />
5. 检查新的 <code>slot</code> 分配信息<br />
<img src="/_resources/c5b46beb6e124feb994296b0dcd601a0.png" /><br />
6. 如集群中需要从节点，可选择对slave扩容<br />
<a title="#%E5%A2%9E%E5%8A%A0slave%E8%8A%82%E7%82%B9" href="#%E5%A2%9E%E5%8A%A0slave%E8%8A%82%E7%82%B9">增加slave节点</a><br />
!!!</p>
<h3 id="集群缩容">集群缩容</h3>
<p>!!! success 场景思路</p>
<blockquote>
<p>场景：随着业务萎缩用户量下降明显，redis服务器负载出现闲置，需要对其进行资源释放<br />
思路：再回收槽位，删除主节点，最后删除从节点。<br />
!!!</p>
</blockquote>
<p>!!! info 实际操作<br />
0. 当前集群情况 (master:4, slave:1)<br />
<img src="/_resources/f3ad8277303c40d5872211bcf89b59fd.png" /><br />
<img src="/_resources/6d25f75aa16f43148f77cd50b385f4ab.png" /></p>
<ol>
<li>
<p>回收槽位</p>
<div><pre class="hljs"><code>redis-<span class="hljs-keyword">cli</span> -a &lt;auth-passwd&gt; --<span class="hljs-keyword">cluster</span> reshard &lt;oneof-<span class="hljs-keyword">cluster</span>-node-ip&gt;:6379</code></pre></div>
<ul>
<li>slot数量
<ul>
<li>需要分配的 <code>slot</code> 数量 (回收主机拥有slot数/回收后master数量 --&gt; 4096/3=1365)</li>
<li>此处使用了均分的方式，因此每次<code>只移动1/3</code>的slot</li>
</ul>
</li>
<li>接受slot的ID (选择用于回收slot的master ID，多master可以任选)
<ul>
<li>用于接收释放的 <code>slot</code></li>
</ul>
</li>
<li>回收slot的策略 (删除写入<code>需要删除的节点</code>--&gt;<code>Done</code>)
<ul>
<li><strong>使用 "all"</strong>：当你想要将集群中所有现有节点都指定为哈希槽的源节点时，你会使用 "all"。
<ul>
<li>这在你<code>向集群添加新节点时很有用</code>，你希望现有节点将它们的哈希槽<code>均匀地分配给它们自己和新节点</code>。</li>
</ul>
</li>
<li><strong>使用 "done"</strong>：当你<code>完成手动输入所有源节点ID的任务</code>时，你会使用 "done"。
<ul>
<li>这表示你已经完成了指定哈希槽重新分配的源节点的任务。</li>
</ul>
</li>
</ul>
</li>
<li>交互式执行 (1次)
<ul>
<li><img src="/_resources/4e95461006de41bb8426ed5920148c21.png" /></li>
</ul>
</li>
<li>非交互形式执行 (2次)<div><pre class="hljs"><code>redis-cli -a &lt;auth-passwd&gt; --cluster reshard 192.168.100.102:6379 \
--cluster-slots 1365 --cluster-from &lt;node-need-to-delete-ID&gt; \
--cluster-to &lt;node-will-receive-slot-ID&gt; --cluster-yes

eg:
<span class="hljs-comment"># 103 接收 101 放弃的 1365 个 slot </span>
redis-cli -a alopex --cluster reshard 192.168.100.102:6379 \
--cluster-slots 1365 --cluster-from af550d85cf75582eb485b7c06924f22a8a1a8a87 \
--cluster-to 56ae4fbb871e4fb8a47cb06ef39d4387751efb60 --cluster-yes

<span class="hljs-comment"># 104 接收 101 放弃的 1366 个 slot </span>
redis-cli -a alopex --cluster reshard 192.168.100.102:6379 \
--cluster-slots 1366 --cluster-from af550d85cf75582eb485b7c06924f22a8a1a8a87 \
--cluster-to ee5da0388305257fee5b553ae56751e10ec344a8 --cluster-yes</code></pre></div>
</li>
</ul>
</li>
<li>
<p>确认slot已被回收</p>
<blockquote>
<p>redis-cli -h 127.0.0.1 -a alopex --no-auth-warning --cluster check 192.168.100.108:6379<br />
可以看到此前的 101 和 108 默认成为了 104 的 slave<br />
<img src="/_resources/93eec57ddea74ef6aee1e06547917777.png" /><br />
<img src="/_resources/455c780d2d934cc2be1858f20df14665.png" /></p>
</blockquote>
</li>
<li>
<p>删除slave节点</p>
<blockquote>
<p>redis-cli -a &lt;\auth-passwd&gt; --cluster del-node &lt;\oneof-cluster-node&gt;:6379 &lt;\slave-node-need-to-del-ID&gt;<br />
<img src="/_resources/8c2d81c82c794eb5ae84d8029dc3f94a.png" /></p>
</blockquote>
</li>
<li>
<p>节点信息更新</p>
<blockquote>
<p>redis-cli -a &lt;\auth-passwd&gt; --no-auth-warning cluster nodes<br />
<img src="/_resources/562a75690aa2485599b3235ae26cbb36.png" /><br />
!!!</p>
</blockquote>
</li>
</ol>
<h3 id="集群倾斜">集群倾斜</h3>
<blockquote>
<p>场景：多个节点运行一段时间后,可能会出现倾斜现象,某个节点数据偏多,内存消耗更大,</p>
</blockquote>
<ul>
<li>
<p>原因：</p>
<ul>
<li>节点和槽分配不均</li>
<li>不同槽对应键值数量差异较大</li>
<li>包含bigkey,建议少用</li>
<li>内存相关配置不一致</li>
<li>热点数据不均衡 : 一致性不高时,可以使用本缓存和MQ</li>
</ul>
</li>
<li>
<p>解决方式</p>
<ul>
<li>执行自动的槽位重新平衡分布,但<code>会影响客户端的访问</code></li>
<li>redis-cli --cluster rebalance &lt;集群节点IP:PORT&gt;</li>
</ul>
</li>
</ul>
</div>
      </article>
    </div>
  </body>
</html>
